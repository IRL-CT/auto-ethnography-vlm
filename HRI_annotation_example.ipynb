{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvW5lIgcYPgi",
        "outputId": "abcfdddb-3b0d-42ae-b155-7879b34163be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai\n",
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyDfRpbW9jf5hY9UpkPkvy_mhWJA2rVBpvs\")"
      ],
      "metadata": {
        "id": "1SHBg5srYoxv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "def upload_video_to_gemini(video_path):\n",
        "    \"\"\"Upload video to Gemini and wait for processing\"\"\"\n",
        "    print(f\"Uploading {video_path}...\")\n",
        "\n",
        "    # Upload the video file\n",
        "    uploaded_file = genai.upload_file(\n",
        "        path=video_path,\n",
        "        display_name=Path(video_path).name\n",
        "    )\n",
        "\n",
        "    print(\"Waiting for video processing...\")\n",
        "    # Wait for processing to complete\n",
        "    while uploaded_file.state.name == \"PROCESSING\":\n",
        "        print(\".\", end=\"\")\n",
        "        time.sleep(2)\n",
        "        uploaded_file = genai.get_file(uploaded_file.name)\n",
        "\n",
        "    if uploaded_file.state.name == \"FAILED\":\n",
        "        raise ValueError(f\"Video processing failed: {uploaded_file.state}\")\n",
        "\n",
        "    print(f\"\\nVideo processed successfully!\")\n",
        "    return uploaded_file\n"
      ],
      "metadata": {
        "id": "0LuuDoKNYrcl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_hri_annotation_prompt():\n",
        "    \"\"\"Create optimized prompt for HRI video analysis\"\"\"\n",
        "    return \"\"\"\n",
        "You are an expert HRI researcher analyzing trash barrel robot interactions in NYC.\n",
        "\n",
        "ANALYZE this entire video and identify ALL human-robot interactions chronologically.\n",
        "\n",
        "For each interaction, provide:\n",
        "- start_time: \"MM:SS\" format when interaction begins\n",
        "- end_time: \"MM:SS\" format when interaction ends\n",
        "- interaction_type: choose from [approaching, avoiding, photographing, pointing, talking, helping, throwing_trash, looking, crowd_formation, no_clear_interaction]\n",
        "- detailed_description: objective description of what happens\n",
        "- person_count: number of people involved\n",
        "- confidence_score: 0.0-1.0 based on clarity of interaction\n",
        "\n",
        "IMPORTANT GUIDELINES:\n",
        "- Include ALL visible interactions, even brief ones (2-3 seconds)\n",
        "- Look for people taking photos, pointing, approaching robot\n",
        "- Note conversations about the robot (use audio cues)\n",
        "- Track if same people interact multiple times\n",
        "- Be precise with timestamps\n",
        "\n",
        "Return as JSON array:\n",
        "[\n",
        "  {\n",
        "    \"start_time\": \"00:15\",\n",
        "    \"end_time\": \"00:23\",\n",
        "    \"interaction_type\": \"photographing\",\n",
        "    \"detailed_description\": \"Woman in blue jacket takes photo of robot with phone\",\n",
        "    \"person_count\": 1,\n",
        "    \"confidence_score\": 0.9\n",
        "  }\n",
        "]\n",
        "\n",
        "If no interactions found, return empty array: []\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JWpsDixCYwrf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_video_with_gemini(video_file, model_name=\"gemini-1.5-flash\"):\n",
        "    \"\"\"Analyze entire video with Gemini VLM\"\"\"\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "    prompt = create_hri_annotation_prompt()\n",
        "\n",
        "    print(\"Analyzing video with Gemini...\")\n",
        "\n",
        "    try:\n",
        "        # generate content with video + prompt\n",
        "        response = model.generate_content([\n",
        "            video_file,\n",
        "            prompt\n",
        "        ])\n",
        "\n",
        "        # extract JSON from response\n",
        "        response_text = response.text\n",
        "        print(\"Raw response:\", response_text[:200] + \"...\" if len(response_text) > 200 else response_text)\n",
        "\n",
        "        # find JSON in response\n",
        "        json_start = response_text.find('[')\n",
        "        json_end = response_text.rfind(']') + 1\n",
        "\n",
        "        if json_start != -1 and json_end > json_start:\n",
        "            json_str = response_text[json_start:json_end]\n",
        "            annotations = json.loads(json_str)\n",
        "            return annotations\n",
        "        else:\n",
        "            print(\"No JSON found in response\")\n",
        "            return []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing video: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "j7HLftQvY0YO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_annotation_dataframe(annotations, video_name):\n",
        "    \"\"\"Convert annotations to DataFrame matching your research format\"\"\"\n",
        "    if not annotations:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    data = []\n",
        "    for ann in annotations:\n",
        "        data.append({\n",
        "            'tcn_layers': video_name,\n",
        "            'Start Time': ann.get('start_time', ''),\n",
        "            'End Time': ann.get('end_time', ''),\n",
        "            'Event': ann.get('interaction_type', ''),\n",
        "            'Observations': ann.get('detailed_description', ''),\n",
        "            'Initiation Action': '',  # Can be filled manually or with follow-up prompts\n",
        "            'Ending Action': '',     # Can be filled manually or with follow-up prompts\n",
        "            'Person Count': ann.get('person_count', 1),\n",
        "            'Confidence': ann.get('confidence_score', 0.0)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "fNMxkZfpY4k9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(video_path, output_path):\n",
        "    \"\"\"Main function to process video and create annotations\"\"\"\n",
        "\n",
        "    # upload video to Gemini\n",
        "    video_file = upload_video_to_gemini(video_path)\n",
        "\n",
        "    # analyze with Gemini VLM\n",
        "    annotations = analyze_video_with_gemini(video_file)\n",
        "\n",
        "    print(f\"Found {len(annotations)} interactions\")\n",
        "\n",
        "    # convert to DataFrame\n",
        "    video_name = Path(video_path).name\n",
        "    df = create_annotation_dataframe(annotations, video_name)\n",
        "\n",
        "    # save to Excel\n",
        "    df.to_excel(output_path, index=False)\n",
        "    print(f\"Annotations saved to {output_path}\")\n",
        "\n",
        "    # display results\n",
        "    print(\"\\nDetected Interactions:\")\n",
        "    for i, ann in enumerate(annotations):\n",
        "        print(f\"{i+1}. {ann['start_time']}-{ann['end_time']}: {ann['interaction_type']} - {ann['detailed_description']}\")\n",
        "\n",
        "    return df, annotations\n",
        "\n",
        "# run the analysis\n",
        "video_path = \"/content/drive/MyDrive/HRI-Annotation/sample_clips/sample_clip.mp4\"\n",
        "output_path = \"/content/drive/MyDrive/HRI-Annotation/sample_annotations_output.xlsx\"\n",
        "\n",
        "# process the video\n",
        "df, annotations = process_video(video_path, output_path)\n",
        "\n",
        "# display the DataFrame\n",
        "print(\"\\nFinal DataFrame:\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eqSeov89Y6gY",
        "outputId": "4fc47acc-60e5-4047-f842-d8edfa694bed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading /content/drive/MyDrive/HRI-Annotation/sample_clips/sample_clip.mp4...\n",
            "Waiting for video processing...\n",
            "........................\n",
            "Video processed successfully!\n",
            "Analyzing video with Gemini...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 35774.30ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw response: ```json\n",
            "[\n",
            "  {\n",
            "    \"start_time\": \"00:24\",\n",
            "    \"end_time\": \"00:26\",\n",
            "    \"interaction_type\": \"throwing_trash\",\n",
            "    \"detailed_description\": \"A person throws a cup into the trash barrel.\",\n",
            "    \"person_coun...\n",
            "Found 10 interactions\n",
            "Annotations saved to /content/drive/MyDrive/HRI-Annotation/annotations_output.xlsx\n",
            "\n",
            "Detected Interactions:\n",
            "1. 00:24-00:26: throwing_trash - A person throws a cup into the trash barrel.\n",
            "2. 00:55-00:57: throwing_trash - A girl throws a juice box into the trash barrel.\n",
            "3. 01:19-01:29: talking - Two people sitting at a table talk to each other about taking photos of the robot. One person takes a picture.\n",
            "4. 02:20-02:31: talking - Two people sitting at a table talk about taking a picture of each other with the robot in the background. One person takes a picture.\n",
            "5. 02:48-02:51: approaching - A woman and a child approach the trash barrel.\n",
            "6. 03:04-03:07: talking - A woman walks towards the robot, talking to a person sitting next to the robot, before walking away.\n",
            "7. 03:16-03:20: talking - Two people stand by the robot, talking to each other.\n",
            "8. 03:20-03:30: talking - A woman and a boy stand by the robot, talking to each other.\n",
            "9. 04:05-04:09: talking - Two people sitting at a table talk to each other, mentioning the robot.\n",
            "10. 04:10-04:13: throwing_trash - A girl throws a plastic bottle into the trash barrel.\n",
            "\n",
            "Final DataFrame:\n",
            "        tcn_layers Start Time End Time           Event  \\\n",
            "0  sample_clip.mp4      00:24    00:26  throwing_trash   \n",
            "1  sample_clip.mp4      00:55    00:57  throwing_trash   \n",
            "2  sample_clip.mp4      01:19    01:29         talking   \n",
            "3  sample_clip.mp4      02:20    02:31         talking   \n",
            "4  sample_clip.mp4      02:48    02:51     approaching   \n",
            "5  sample_clip.mp4      03:04    03:07         talking   \n",
            "6  sample_clip.mp4      03:16    03:20         talking   \n",
            "7  sample_clip.mp4      03:20    03:30         talking   \n",
            "8  sample_clip.mp4      04:05    04:09         talking   \n",
            "9  sample_clip.mp4      04:10    04:13  throwing_trash   \n",
            "\n",
            "                                        Observations Initiation Action  \\\n",
            "0       A person throws a cup into the trash barrel.                     \n",
            "1   A girl throws a juice box into the trash barrel.                     \n",
            "2  Two people sitting at a table talk to each oth...                     \n",
            "3  Two people sitting at a table talk about takin...                     \n",
            "4     A woman and a child approach the trash barrel.                     \n",
            "5  A woman walks towards the robot, talking to a ...                     \n",
            "6  Two people stand by the robot, talking to each...                     \n",
            "7  A woman and a boy stand by the robot, talking ...                     \n",
            "8  Two people sitting at a table talk to each oth...                     \n",
            "9  A girl throws a plastic bottle into the trash ...                     \n",
            "\n",
            "  Ending Action  Person Count  Confidence  \n",
            "0                           1         1.0  \n",
            "1                           1         1.0  \n",
            "2                           2         0.8  \n",
            "3                           2         0.8  \n",
            "4                           2         1.0  \n",
            "5                           2         0.7  \n",
            "6                           2         0.7  \n",
            "7                           2         0.7  \n",
            "8                           2         0.6  \n",
            "9                           1         1.0  \n"
          ]
        }
      ]
    }
  ]
}